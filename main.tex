\documentclass[14pt,a4paper]{extreport}

% --- Пакетна частина (Преамбула) ---
\usepackage[utf8]{inputenc}
\usepackage[ukrainian]{babel}
\usepackage[T2A]{fontenc}

% --- Пакет Tikz для малювання схем
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing, shapes.geometric}

% --- Пакет Pgfplots для графіків функцій
\usepackage{pgfplots}

\usepackage{amsmath}

\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{xcolor}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{blue},   % Ключові слова без жирного, щоб уникнути відсутнього шрифту T2A/pcr/b/n
    columns=flexible,            % Уникає розриву UTF-8 послідовностей при вирівнюванні колонок
    breaklines=true,         % ДОЗВОЛЯЄ ПЕРЕНОС РЯДКІВ
    breakatwhitespace=true,  % Переносити тільки на пробілах, щоб уникнути розриву UTF-8 послідовностей
    inputencoding=utf8,      % Підтримка UTF-8 у лістингах
    extendedchars=true,      % Підтримка кирилиці
    frame=single,            % Рамка навколо коду
    numbers=left,            % Нумерація рядків зліва
    numberstyle=\tiny,
    showstringspaces=false,  
    tabsize=4,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}, % Символ стрілочки при переносі
}

% Налаштування полів згідно з методичкою (стор. 11): ліве - 3 см, інші - 2 см 
\usepackage[left=30mm, right=20mm, top=20mm, bottom=20mm]{geometry}

\usepackage{times} % Основний шрифт Times New Roman
\usepackage{setspace}
\usepackage{indentfirst} % Абзацний відступ для першого рядка
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage[none]{hyphenat} % Забороняємо переноси всюди за замовчуванням
\sloppy

% Команда для ввімкнення переносів лише там, де ми хочемо
\newcommand{\enablehyphens}{\hyphenpenalty=50 \exhyphenpenalty=50 \pretolerance=10 \tolerance=2000}

% --- Налаштування нумерації (ПРАВИЙ ВЕРХНІЙ КУТ) ---
\usepackage{fancyhdr}
\setlength{\headheight}{17pt}
\addtolength{\topmargin}{-3pt}
\pagestyle{fancy}
\fancyhf{} 
\fancyhead[R]{\thepage} % Номер сторінки справа вгорі
\renewcommand{\headrulewidth}{0pt}

% Переозначуємо стиль plain, щоб номер був справа вгорі навіть на сторінках з заголовками
\fancypagestyle{plain}{
    \fancyhf{}
    \fancyhead[R]{\thepage}
    \renewcommand{\headrulewidth}{0pt}
}

% Налаштування абзацного відступу (1.25 см)
\setlength{\parindent}{1.25cm}
\onehalfspacing % Міжрядковий інтервал 1.5

% Налаштування заголовків (ВЕЛИКИМИ ЛІТЕРАМИ по центру) 
\titleformat{\chapter}[display]
  {\normalfont\large\bfseries\centering}{}{0pt}{\large\MakeUppercase}
\titlespacing*{\chapter}{0pt}{-20pt}{20pt}

\begin{document}

% --- СТОРІНКА 1: ТИТУЛЬНА (Номер не ставиться) ---
\begin{titlepage}
    \thispagestyle{empty}
    \centering
    \small
    МІНІСТЕРСТВО ОСВІТИ І НАУКИ УКРАЇНИ\\
    ДЕРЖАВНИЙ ВИЩИЙ НАВЧАЛЬНИЙ ЗАКЛАД\\
    «УЖГОРОДСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ»\\
    ФАКУЛЬТЕТ МАТЕМАТИКИ ТА ЦИФРОВИХ ТЕХНОЛОГІЙ\\
    Кафедра системного аналізу і теорії оптимізації
    
    \vfill
    \Large \textbf{ВОГАР АНДРІЙ ЮРІЙОВИЧ}
    
    \vspace{1cm}
    \Large \textbf{ОПТИМІЗАЦІЯ ВЕЛИКИХ МОВНИХ МОДЕЛЕЙ ДЛЯ РЕСУРС-ОБМЕЖЕНИХ ОБЧИСЛЮВАЛЬНИХ СИСТЕМ}
    
    \vfill
    \large 124 Системний аналіз
    
    \vfill
    \textbf{Дипломна робота на здобуття освітнього ступеня бакалавра}
    
    \vfill
    \begin{flushright}
        \begin{minipage}{0.5\textwidth}
            \textbf{Науковий керівник:} \\
            Корник Олександр Володимирович \\
            асистент каф. САТО
        \end{minipage}
    \end{flushright}
    
    \vfill
    \textbf{Ужгород – 2026}
\end{titlepage}

% --- СТОРІНКИ 2-4: МІСЦЕ ДЛЯ ІНШИХ МАТЕРІАЛІВ ---
% Якщо у вас тут будуть інші сторінки, просто додавайте їх перед Змістом.
% Зараз я використовую команду \setcounter{page}{5} нижче.

\newpage

\newpage
\chapter*{АНОТАЦІЯ}
Це дослідження присвячене аналізу та розробці методів підвищення обчислювальної ефективності великих мовних моделей (LLM) для їхнього розгортання на пристроях з обмеженими апаратними ресурсами.

У роботі розглянуто сучасний стан розвитку нейронних мереж та виявлено основні бар'єри, що заважають локальному використанню потужних моделей на мобільних і вбудованих системах. Детально розглянуто та систематизовано основні методи стиснення моделей: квантування (Quantization); прунінг (Pruning); а також використання ефективних архітектур, таких як BitNet та дистильовані моделі.

Об'єктом дослідження є процеси оптимізації великих мовних моделей. Предметом дослідження виступають алгоритми квантування, прунінгу та архітектурні модифікації, що дозволяють зменшити обчислювальні витрати без критичної втрати точності генерації. Практична цінність роботи полягає у визначенні найбільш ефективних конфігурацій оптимізації для забезпечення балансу між швидкістю роботи моделі (tokens per second) та її інтелектуальними можливостями на слабких обчислювальних системах.

\vspace{0.5cm}
\noindent \textbf{Ключові слова:} великі мовні моделі (LLM), оптимізація нейронних мереж, квантування, прунінг, ресурс-обмежені системи, BitNet, штучний інтелект на пристроях (Edge AI).

\newpage
\chapter*{ABSTRACT}
\begin{otherlanguage}{english}
This research is dedicated to the analysis and development of methods for increasing the computational efficiency of Large Language Models (LLMs) for their deployment on devices with limited hardware resources.

The study examines the current state of neural network development and identifies the primary barriers preventing the local use of powerful models on mobile and embedded systems. Key model compression methods are detailed and systematized: quantization (Post-Training Quantization and Quantization-Aware Training); pruning (structured and unstructured approaches); and the implementation of efficient architectures, such as BitNet and distilled models.

The object of the study is the process of optimizing large language models. The subject of the study involves quantization algorithms, pruning techniques, and architectural modifications that allow for the reduction of computational overhead without a critical loss in generation accuracy. The practical value of the work lies in determining the most effective optimization configurations to achieve a balance between inference speed (tokens per second) and the model's intellectual performance on resource-constrained computing systems.


\vspace{0.5cm}
\noindent \textbf{Keywords:} Large Language Models (LLM), neural network optimization, quantization, pruning, resource-constrained systems, BitNet, Edge AI.
\end{otherlanguage}

% --- СТОРІНКА 5: ЗМІСТ ---
\newpage
\pagenumbering{arabic}
\setcounter{page}{5} % Встановлюємо, що ЗМІСТ - це 5-та сторінка

\chapter*{ЗМІСТ}
\makeatletter
\@starttoc{toc}
\makeatother

% --- СТОРІНКА 6: ВСТУП ---
\newpage

\chapter*{ВСТУП}
\addcontentsline{toc}{chapter}{ВСТУП}

Великі мовні моделі (Large Language Models, LLMs) у сучасному світі стали невід'ємною частиною життя для багатьох людей, що використовують штучний інтелект. Вони вкючають у себе обробку природньої мови, машинний переклад, генерацію тексту та інше. Проте, через величезну кількість параметрів, що входять у мовну модель створюються великі обчислювальні вимоги. Запуск таких моделей на ресурс-обмежених пристроях, таких як мобільні телефони або вбудовані системи, стає викликом.

Аналіз та оптимізація мовних моделей є критично важливим для забезпечення їхньої доступності, масштабованості та ефективності на різних платформах. Квантування (Qantinization), прунінг (pruning) та розробка ефективних архітектур є основними  у цій галузі, які дозволяють зменшити розмір самих моделей, зменшити вимоги до заліза та за рахунок цього покращити їх продуктивність без значної втрати якості виводу моделі.

Кінцевою метою є оптимізація вже наявних мовних моделей для роботи на слабких пристроях при одночасному збереженні їхньої функціональності, точності та якості, що сприятиме широкому впровадженні штучного інтелекту в повсякденне життя.

Об'єкт дослідження: Процеси оптимізації великих мовних моделей.

Предмет дослідження: методи квантування, прунінгу та розробки ефективних архітектур для запуску великих мовних моделей на ресурс-обмежених пристроях.

Мета дослідження: Розробка та впровадження ефективних методів оптимізації великих мовних моделей для їхнього запуску на слабких пристроях.

Це дослідження може бути корисним для дослідників та пересічної зацікавленої людини у галузі штучного інтелекту, машинного навчання, роботи мовних моделей. Також для інженерів, які працюють та шукають шляхи для впровадженням великих мовних моделей у реальні застосунки на малопотужних пристроях.


% --- РОЗДІЛ 1. Теоретична частина ---

\newpage
% Згідно з методичкою (стор. 12), заголовки розділів ВЕЛИКИМИ ЛІТЕРАМИ
\chapter{РОЗДІЛ 1. ТЕОРЕТИЧНІ ОСНОВИ ОПТИМІЗАЦІЇ ВЕЛИКИХ МОВНИХ МОДЕЛЕЙ}

\section{Концепція та архітектурні особливості великих мовних моделей}
% Тут ви описуєте LLM, увагу (Attention) та параметри.

\subsection{Визначення та еволюція мовних моделей}
% Тут варто дати базове визначення: що таке LLM, як вони навчаються на величезних масивах текстових даних для передбачення наступного токена. Опишіть перехід від рекурентних мереж (RNN/LSTM) до архітектури Transformer, яка стала фундаментом для сучасних моделей.

LLM (Large Language Model) - це велика мовна модель, яка є типом штучного інтелекту, навченого розуміти та генерувати людську мову [1, с. 4].

Історію розвитку великих мовних моделей доцільно розділити на кілька ключових етапів відповідно до технологічних підходів, що домінували у певні періоди:

\begin{enumerate}
    \item \textbf{Статистичні методи та прості вектори (до 2013 р.):}
    \begin{itemize}
        \item \textit{Bag-of-Words (BoW):} ранні підходи представляли текст як «мішок слів», підраховуючи частоту їх появи. Це дозволяло перетворити текст на числові дані, проте втрачався порядок слів та їхній семантичний контекст [1, с. 6].
        \item \textit{N-gram:} моделі передбачали наступне слово, враховуючи лише $N$ попередніх одиниць тексту. Це було ефективно для вузьких завдань, але не дозволяло опрацьовувати довгі контекстні зв'язки [1, с. 88].
    \end{itemize}

    \item \textbf{Векторні представлення слів (2013 р.):}
    \begin{itemize}
        \item \textit{Word2Vec:} технологічна революція, що дозволила створювати вбудовування (embeddings) – щільні вектори, де слова зі схожим значенням розташовані поруч у математичному просторі. Наприклад, векторні операції дозволяли розв'язувати аналогії типу «король – чоловік + жінка = королева» [1, с. 8]. Проте кожне слово мало лише один фіксований вектор (наприклад, слово «bank» мало однакове значення і для фінансової установи, і для берега річки) [1, с. 11].
    \end{itemize}

    \item \textbf{Рекурентні мережі та контекст (2014--2016 рр.):}
    \begin{itemize}
        \item \textit{RNN та LSTM:} ці архітектури обробляли текст послідовно, зберігаючи «пам’ять» про попередні токени. Це дозволило враховувати порядок слів і локальний контекст [1, с. 11].
        \item \textit{Увага (Attention):} механізм, що дозволив моделям фокусуватися на найбільш релевантних словах у реченні при виконанні завдань (наприклад, машинного перекладу), замість стиснення всього речення в один вектор [1, с. 13].
    \end{itemize}

    \item \textbf{Ера Трансформерів (2017 р. – теперішній час):}
    \begin{itemize}
        \item \textit{Transformer (2017):} публікація статті «Attention Is All You Need» представила архітектуру, яка повністю відмовилася від рекурентності на користь механізму самоуваги (Self-Attention). Це дозволило обробляти всі слова паралельно, що значно пришвидшило навчання на великих масивах даних та заклало фундамент для сучасних LLM [1, с. 15].
    \end{itemize}
\end{enumerate}

Сучасні LLM є переважно генеративними моделями (decoder-only) та побудовані на архітектурі трансформерів (Transformers). Вони працюють за принципом авторегресії: модель отримає на вхід текст і намагається передбачити, яке слово (або частина слова) має йти наступним [1, с. 12].

Сприйняття тексту моделлю відрізняється від людського: на вхід подається не символьний ряд, а послідовність числових значень. Для них текст розбивається на менші одиниці - токени (слова, частини слів). Кожен токен перетворюється на числовий вектор, який модель може обробляти математично. Процес перетворення тексту на токени називається ембеддингом (embedding) [1, с. 6]. Розглянемо приклад речення «Я люблю пити Кока-Колу», яке розбивається на окремі токени:

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Текст}  & Я & люблю & пити & Кока & - & Колу \\ \hline
        \textbf{Токен}  & [251] & [14321] & [842] & [3210] & [12] & [4512] \\ \hline
    \end{tabular}
    \caption{Приклад токенізації речення}
    \label{table:tokenization}
\end{table}

Слід зауважити, що ідентифікатори токенів залежать від словника конкретної моделі (наприклад, GPT-5 або Llama-4).

Завдяки механізму уваги (self-attention), модель здатна враховувати котекст усього речення, абзацу та більше, розуміючи зв'язки між словами, навіть якщо вони далеко одне від одного [1, с. 16].

Процес навчання у мовних моделей називається попереднім навчанням (pretraining), оскільки вона не навчається в процесі роботи, а тільки перед початком використання. Модель навчається на величезних масивах тексту (книги, веб-сайти), вивчаючи граматику, факти про світ та логічні зв'язки без участі вчителя (self-supervised learning) [1, с. 26].


\subsection{Архітектура Transformer та механізм самоуваги (Self-Attention)}
% Це ядро вашої теоретичної частини. Поясніть, як механізм Multi-Head Attention дозволяє моделі обробляти контекст паралельно, а не послідовно. Це важливо, бо саме обчислення "уваги" створює основне навантаження на ресурси (особливо на пам'ять при великій довжині контексту).

Архітектура Transformer, представлена у 2017 році в статті «Attention Is All You Need», стала фундаментом для сучасних великих мовних моделей (LLM) [2]. Вона замінила рекурентні мережі (RNN), дозволивши тренувати моделі паралельно та значно ефективніше обробляти довгі послідовності даних.

Архітектура Трансформера сладається з двох блоків: енкодера (encoder) та декодера (decoder) складених один на одного, як показано на рис. 1.1.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=1.5cm,
        terminal/.style={rectangle, rounded corners, draw, fill=gray!10, text width=5cm, align=center, font=\small},
        process/.style={rectangle, draw, thick, fill=blue!10, text width=5cm, align=center, minimum height=1cm, font=\bfseries},
        vector/.style={ellipse, draw, fill=orange!10, font=\itshape\small},
        arrow/.style={thick, ->, >=stealth}
    ]
        % Вузли схеми
        \node (input) [terminal] {Вхід: [Я] [люблю] [Кока-Колу]};
        \node (encoder) [process, below of=input] {ENCODER \\ (Аналіз контексту)};
        \node (context) [vector, below of=encoder] {Контекстний вектор};
        \node (decoder) [process, below of=context] {DECODER \\ (Генерація відповіді)};
        \node (output) [terminal, below of=decoder] {Вихід: [I] [love] [Coca-Cola]};

        % Стрілки
        \draw [arrow] (input) -- (encoder);
        \draw [arrow] (encoder) -- (context);
        \draw [arrow] (context) -- (decoder);
        \draw [arrow] (decoder) -- (output);

    \end{tikzpicture}
    \caption{Спрощена схема взаємодії Encoder та Decoder в архітектурі Transformer}
    \label{fig:transformer_logic}
\end{figure}

Архітектура сучасних моделей базується на послідовному накладанні однакових структурних елементів. Основними будівельними одиницями виступають блоки трансформера (Transformer Blocks), які повторюються у структурі моделі велику кількість разів. Наприклад, у моделі GPT-3 кількість таких блоків (шарів) сягає 96 [3].

Кожен такий блок складається з двох основних частин:
\begin{enumerate}
    \item \textit{Шар уваги} (Attention Layer) - дозволяє моделі фокусуватися на різних частинах вхідної послідовності.
    \item \textit{Нейронна мережа прямого поширення} (Feedforward Neural Network - FFN/MLP) - здійснює обробку отриманих векторів.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=1.2cm,
        block/.style={rectangle, draw, fill=blue!10, text width=6cm, align=center, minimum height=0.8cm, font=\small},
        arrow/.style={thick, ->, >=stealth}
    ]
        % Вузли схеми
        \node (output) [block, fill=green!10] {\textbf{Вихід (наступний шар)}};
        \node (norm2) [block, above of=output, node distance=1.5cm] {Layer Normalization};
        \node (ff) [block, above of=norm2] {Feed-Forward Neural Network};
        \node (norm1) [block, above of=ff] {Layer Normalization};
        \node (att) [block, above of=norm1] {Multi-Head Attention Layer};
        \node (input) [block, above of=att, fill=orange!10] {\textbf{Вхідні ембеддинги / Попередній шар}};

        % Стрілки (рух знизу вгору згідно з вашою логікою)
        \draw [arrow] (input) -- (att);
        \draw [arrow] (att) -- (norm1);
        \draw [arrow] (norm1) -- (ff);
        \draw [arrow] (ff) -- (norm2);
        \draw [arrow] (norm2) -- (output);

        % Декоративна дужка "x96"
        \draw [decorate, decoration={brace, amplitude=10pt, mirror}] 
            (input.south west) -- (output.north west) 
            node [midway, left, xshift=-0.5cm, font=\bfseries] {x96};

    \end{tikzpicture}
    \caption{Схематичне представлення ієрархічної структури блоків у моделі GPT-3}
    \label{fig:gpt3_arch}
\end{figure}

Механізм самоуваги (Self-Attention) дозволяє моделі фокусуватися на різних частинах вхідної послідовності під час обробки кожного токена. Це забезпечує глибоке розуміння контексту: наприклад, у реченні «Собака побіг за білкою, бо \textbf{вона}...» механізм уваги допомагає визначити, чи стосується займенник «вона» собаки чи білки, аналізуючи синтаксичні та семантичні зв'язки [1, с. 88].

Математично цей процес базується на трьох проекційних матрицях, що створюються під час навчання: Запит ($Q$ – Query), Ключ ($K$ – Key) та Значення ($V$ – Value). Основна операція обчислення уваги виражається формулою:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

де $d_k$ – розмірність векторів ключів, що використовується для масштабування, а функція \text{softmax} нормалізує отримані бали, перетворюючи їх на ймовірності, сума яких дорівнює одиниці.

Функція \text{softmax} відіграє ключову роль у механізмі уваги, оскільки вона перетворює вектор довільних дійсних чисел $z$ у вектор ймовірностей, де кожен елемент знаходиться в діапазоні $(0, 1)$. Математично вона визначається як:

\begin{equation}
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\end{equation}

де $z_i$ – вхідний бал для $i$-го токена, а знаменник є сумою експонент усіх вхідних балів $K$ токенів у послідовності. Це забезпечує те, що модель приділяє найбільшу «увагу» найбільш релевантним елементам, пригнічуючи шум від менш важливих токенів.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            axis lines = left,
            xlabel = $z_i$,
            ylabel = {$\sigma(z)_i$},
            xmin=-5, xmax=5,
            ymin=0, ymax=1,
            grid=both,
            grid style={line width=.1pt, draw=gray!10},
            major grid style={line width=.2pt, draw=gray!50},
            width=10cm, height=6cm,
            title={Функція Softmax для двох значень ($z_1, z_2$)}
        ]
            % Графік Softmax для z1, при фіксованому z2 = 0
            \addplot [
                domain=-5:5, 
                samples=100, 
                color=blue,
                thick
            ]
            {exp(x)/(exp(x) + exp(0))};
            \addlegendentry{$\frac{e^{z_1}}{e^{z_1} + e^0}$}
        \end{axis}
    \end{tikzpicture}
    \caption{Графічна інтерпретація функції Softmax}
    \label{fig:softmax_graph}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm,
        auto,
        block/.style={rectangle, draw, thick, minimum width=6cm, align=center, rounded corners=2pt},
        data/.style={rectangle, draw, fill=gray!5, minimum width=6cm, align=center, font=\small},
        arrow/.style={thick, ->, >=stealth}
    ]
        % Вхідні логіти з візуалізацією
        \node (input) [data] {
            \textbf{Вхідні логіти ($z$)} \\
            $[10.0, \quad 5.0, \quad 1.0]$ \\
            \begin{tikzpicture}[scale=0.3]
                \fill[blue!40] (0,0) rectangle (3,0.8);
                \fill[blue!40] (4,0) rectangle (5.5,0.8);
                \fill[blue!40] (8,0) rectangle (8.5,0.8);
            \end{tikzpicture}
        };
        
        % Блок Softmax з формулою
        \node (softmax) [block, fill=blue!10, below of=input] {
            \textbf{Функція Softmax} \\
            $\sigma(z)_i = \frac{e^{z_i}}{\sum e^{z_j}}$
        };
        
        % Вихідні ймовірності з акцентом на домінування
        \node (output) [data, fill=green!5, below of=softmax] {
            \textbf{Ймовірності (Probabilities)} \\
            $[0.993, \quad 0.006, \quad 0.001]$ \\
            \begin{tikzpicture}[scale=0.3]
                \fill[green!60!black] (0,0) rectangle (9,0.8);
                \fill[green!60!black] (10,0) rectangle (10.1,0.8);
                \fill[green!60!black] (11,0) rectangle (11.05,0.8);
            \end{tikzpicture}
        };
        
        % Стрілки
        \draw [arrow] (input) -- (softmax);
        \draw [arrow] (softmax) -- (output);
    \end{tikzpicture}
    \caption{Схема перетворення логітів у ймовірнісний розподіл функцією Softmax}
    \label{fig:softmax_detailed}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, every node/.style={fill=white, font=\small}]
        % Спрощена візуалізація Softmax
        \node (token1) [draw, rectangle] {Собака (0.1)};
        \node (token2) [draw, rectangle, right of=token1, xshift=1.2cm] {побіг (0.05)};
        \node (token3) [draw, rectangle, right of=token2, xshift=1.2cm] {білкою (0.8)};
        \node (token4) [draw, rectangle, right of=token3, xshift=1.2cm] {вона (0.05)};
        
        % Стрілка зв'язку
        \path [draw, ->, line width=1pt, bend left] (token3) edge node [above, font=\tiny] {max weight} (token4);
    \end{tikzpicture}
    \caption{Візуалізація розподілу ваг уваги після функції Softmax для слова «вона»}
    \label{fig:softmax_viz}
\end{figure}

Для того щоб модель могла вловлювати різні типи зв'язків одночасно (наприклад, граматичні та смислові), використовується багатоголова увага (Multi-Head Attention). У цьому підході механізм самоуваги дублюється декілька разів. Кожна «голова» працює паралельно з власним набором матриць $Q, K, V$, після чого результати об'єднуються (конкатенуються) [1, с. 90].


\subsection{Параметри моделі та вагові коефіцієнти}
% Поясніть, що таке "параметри" (ваги та зміщення). Наведіть приклад: модель з 7 мільярдами параметрів (7B) зберігає величезну матрицю чисел, кожне з яких визначає силу зв'язку між нейронами. Саме кількість параметрів визначає інтелектуальні можливості моделі, але й прямо пропорційно впливає на вимоги до ОЗП.

Параметри — це внутрішні змінні моделі, які вона оптимізує в процесі навчання на масивах даних. Вони визначають функціональне перетворення вхідних даних у вихідні передбачення [1, с. 8]. Математично параметри сучасних архітектур складаються з двох основних компонентів: ваг ($w$) та зміщень ($b$).

Вагові коефіцієнти (Weights, $w$) становлять переважну більшість параметрів нейронної мережі. У сучасних архітектурах типу Transformer процес їх функціонування та зберігання має такі особливості [1, с. 8]:

\begin{itemize}
    \item \textbf{Принцип роботи:} ваги організовані у вигляді проекційних матриць, зокрема матриць Запиту ($W_Q$), Ключа ($W_K$), Значення ($W_V$) та матриць повнозв’язних шарів прямого поширення (Feed-Forward layers). Вхідні дані у вигляді токенів множаться на ці матриці для формування нових контекстних представлень (рис. 1.6).
    
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[scale=0.8]
            % Матриця входу
            \draw[fill=blue!10] (0,0) rectangle (1,3) node[midway, rotate=90] {Input ($X$)};
            \node at (1.5, 1.5) {$\times$};
            % Матриця ваг
            \draw[fill=orange!20] (2,0) rectangle (7,3) node[midway] {Weights ($W_Q$)};
            \node at (7.5, 1.5) {$=$};
            % Матриця результату
            \draw[fill=green!10] (8,0) rectangle (9,3) node[midway, rotate=90] {Output};
        \end{tikzpicture}
        \caption{Візуалізація процесу лінійного перетворення вхідних даних через матрицю ваг}
        \label{fig:matrix_mult}
    \end{figure}

    \item \textbf{Формати зберігання:} традиційно ваги зберігаються як числа з плаваючою комою (floating-point numbers), наприклад, у форматах FP32 (32-біт) або FP16 (16-біт). Це забезпечує високу точність обчислень, проте вимагає значного обсягу пам'яті та обчислювальних потужностей.
\end{itemize}

Зміщення (Biases, $b$) — додаткові змінні, які дозволяють зміщувати результат активації нейрона, аналогічно до вільного члена в лінійному рівнянні $y = wx + b$ [4]. 

Масштаб сучасних моделей вимагає колосальних обчислювальних ресурсів. Наприклад, якщо GPT-1 налічувала 117 мільйонів параметрів, то GPT-3 вже має 175 мільярдів [1, с. 20]. Популярні моделі середнього розміру (наприклад, Llama-3-7B) містять близько 7 мільярдів параметрів. Саме ця кількість визначає інтелектуальні можливості моделі, але прямо пропорційно впливає на вимоги до обсягу оперативної пам'яті (ОЗП). Для зберігання моделі 7B у форматі FP32 потрібно близько 28 ГБ відеопам'яті, що робить її недоступною для більшості споживчих пристроїв без методів оптимізації.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar,
            symbolic x coords={GPT-1, GPT-2, GPT-3},
            xtick=data,
            ylabel={Параметри (млрд)},
            nodes near coords,
            nodes near coords align={vertical},
            width=10cm, height=6cm,
            bar width=1cm,
            fill=blue!30
        ]
            \addplot coordinates {(GPT-1, 0.117) (GPT-2, 1.5) (GPT-3, 175)};
        \end{axis}
    \end{tikzpicture}
    \caption{Порівняння кількості параметрів у різних поколіннях моделей GPT}
    \label{fig:gpt_scaling}
\end{figure}

\subsection{Представлення даних: від FP32 до цілочисельних типів}
% Опишіть формати збереження ваг. Це критична точка для переходу до практичної частини:
% FP32 (Single Precision): Стандарт для навчання, де кожна вага займає 4 байти.
% FP16/BF16 (Half Precision): Стандарт для сучасного виводу (inference), 2 байти на вагу.
% INT8 / INT4: Цілочисельні формати, що використовуються при квантуванні для стиснення моделі.
% Поясніть концепцію динамічного діапазону та точності (precision).

Перехід від чисел з плаваючою комою високої розрядності до цілочисельних типів зниженої точності є основою методів квантування (Quantization). Це критично важливо для оптимізації пропускної здатності пам'яті та енергоефективності при розгортанні LLM на пристроях з обмеженими ресурсами.

Еволюцію та градацію форматів представлення даних можна структурувати наступним чином:

\begin{enumerate}
    \item \textbf{FP32 (Single-Precision Floating-Point):} стандарт для етапу навчання. Кожен параметр займає 4 байти (32 біти), що дозволяє зберігати значення з високою точністю та широким динамічним діапазоном [1, с. 201]. Проте для моделі з 7 млрд параметрів це потребує близько 28 ГБ пам'яті, що перевищує можливості більшості споживчих систем.
    
    \item \textbf{FP16 та BF16 (Half-Precision):} проміжний етап, де розмір параметра зменшується до 2 байтів (16 бітів). Формат bfloat16 (Brain Floating Point) став стандартом для сучасних LLM, оскільки він зберігає той самий динамічний діапазон, що й FP32, жертвуючи лише точністю мантиси.
    
    \item \textbf{INT8 (8-бітні цілі числа):} квантування дозволяє відобразити (map) континуум значень FP32 у дискретний діапазон цілих чисел від -128 до 127. Це зменшує об’єм моделі у 4 рази порівняно з FP32. При цьому цілочисельна арифметика виконується процесорами (CPU/NPU) значно швидше за арифметику з плаваючою комою [4].
    
    \item \textbf{INT4 та спеціалізовані формати (QLoRA):} зниження розрядності до 4 біт дозволяє зберігати ваги моделі 7B у межах 3.5–4 ГБ VRAM [1, с. 367]. У методах типу QLoRA використовується нормалізований 4-бітний формат (NF4), що дозволяє підтримувати високу якість генерації при радикальному стисненні.
    
    \item \textbf{Тернарні та бінарні формати (BitNet 1.58b):} екстремальний підхід, де ваги обмежуються значеннями $\{-1, 0, 1\}$. Це потребує лише 1.58 біта на параметр і замінює енерговитратне множення матриць на операції додавання та віднімання, що є революційним для мобільних систем [6].
\end{enumerate}

Для наочного порівняння ефективності різних форматів нижче наведено таблицю \ref{table:memory_req}, що демонструє вимоги до пам'яті для моделі об'ємом 7 мільярдів параметрів.

\begin{table}[h!]
    \small
    \centering
    \caption{Вплив формату даних на споживання пам'яті моделі 7B}
    \label{table:memory_req}
    \begin{tabular}{|l|c|c|l|}
        \hline
        \textbf{Тип даних} & \textbf{Біти} & \textbf{Об'єм (ГБ)} & \textbf{Сфера застосування} \\ \hline
        FP32 & 32 & $\sim$28.0 & Навчання (Training) \\ \hline
        FP16 / BF16 & 16 & $\sim$14.0 & Стандартний інференс \\ \hline
        INT8 & 8 & $\sim$7.0 & Квантування для серверів \\ \hline
        INT4 & 4 & $\sim$3.5 & Споживчі ПК та смартфони \\ \hline
        BitNet & 1.58 & $\sim$1.4 & Спеціалізовані пристрої \\ \hline
    \end{tabular}
\end{table}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=0.1cm, font=\tiny]
        % FP32
        \node at (-1.5, 0.5) {\small FP32};
        \draw[fill=red!20] (0,0) rectangle (0.4, 1) node[midway] {S};
        \draw[fill=blue!20] (0.4,0) rectangle (3.2, 1) node[midway] {Exponent (8 bit)};
        \draw[fill=green!20] (3.2,0) rectangle (10, 1) node[midway] {Mantissa (23 bit)};
        
        % INT8
        \node at (-1.5, -1) {\small INT8};
        \draw[fill=red!20] (0,-1.5) rectangle (0.4, -0.5) node[midway] {S};
        \draw[fill=yellow!20] (0.4,-1.5) rectangle (2.8, -0.5) node[midway] {Integer (7 bit)};
        
        \draw[<->, dashed] (0, -0.2) -- (10, -0.2) node[midway, below] {32 bits (4 bytes)};
        \draw[<->, dashed] (0, -1.7) -- (2.8, -1.7) node[midway, below] {8 bits (1 byte)};
    \end{tikzpicture}
    \caption{Порівняння фізичного об'єму пам'яті форматів FP32 та INT8}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[scale=0.8]
        \draw[->] (-0.5,0) -- (5,0) node[right] {FP32 Value};
        \draw[->] (0,-0.5) -- (0,4) node[above] {INT8 Level};
        
        % Крива
        \draw[gray, smooth] (0,0) .. controls (1,0.5) and (2,3.5) .. (4,4);
        
        % Сходинки квантування
        \draw[blue, thick] (0,0) -- (0.5,0) -- (0.5,0.5) -- (1,0.5) -- (1,1.5) -- (2,1.5) -- (2,3) -- (3,3) -- (3,4) -- (4,4);
        
        \node[blue, anchor=west] at (4,3.5) {Quantized (Steps)};
        \node[gray, anchor=west] at (4,1) {Original (Continuous)};
    \end{tikzpicture}
    \caption{Візуалізація дискретизації значень при переході до цілочисельного формату}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        % Налаштування стилів елементів
        block/.style={draw, rectangle, align=center, minimum width=3cm, minimum height=1cm, fill=blue!5, font=\small},
        bitblock/.style={draw, rectangle, align=center, minimum width=3cm, minimum height=1cm, fill=green!5, font=\small},
        val/.style={draw, circle, minimum size=1cm, font=\small},
        op/.style={font=\Large\bfseries},
        label_style/.style={font=\small\bfseries}
    ]

    % --- Верхній ряд (Традиційне обчислення) ---
    \node (label1) [label_style, color=red!70!black] at (3, 1.5) {Heavy Multiplication (Стандарт)};
    
    \node (fp) [block] at (0,0) {FP16 Weight \\ $0.00234$};
    \node (op1) [op] at (3,0) {$\times$};
    \node (in1) [val] at (6,0) {Input};
    
    \draw [thick, ->] (fp) -- (op1);
    \draw [thick, <-] (op1) -- (in1);

    % --- Нижній ряд (BitNet) ---
    \node (label2) [label_style, color=green!50!black] at (3, -1.5) {Lightweight Addition (BitNet)};
    
    \node (bit) [bitblock] at (0,-3) {BitNet Weight \\ $\{-1, 0, 1\}$};
    \node (op2) [op] at (3,-3) {$\pm$};
    \node (in2) [val] at (6,-3) {Input};
    
    \draw [thick, ->] (bit) -- (op2);
    \draw [thick, <-] (op2) -- (in2);

    \end{tikzpicture}
    \caption{Концептуальна різниця між стандартним обчисленням та тернарним підходом BitNet}
    \label{fig:bitnet_comparison}
\end{figure}

\section{Аналіз потреб LLM у ресурсах обчислювальних систем}

\subsection{Математична модель оцінки ресурсних потреб LLM}

Для аналізу ефективності розгортання моделей необхідно попередньо визначити теоретичний об'єм необхідної пам'яті ($M$) та обчислювальну складність інференсу. Об'єм оперативної пам'яті, необхідний для зберігання ваг моделі, можна розрахувати за формулою:

\begin{equation}
M \approx \frac{P \times B}{8 \times 1024^3} + M_{ctx} \quad [\text{ГБ}]
\end{equation}

де:
\begin{itemize}
    \item $P$ — кількість параметрів моделі (наприклад, $8 \times 10^9$);
    \item $B$ — кількість бітів на один параметр (FP32 = 32, FP16 = 16, INT4 = 4);
    \item $M_{ctx}$ — додаткова пам'ять для контекстного вікна (KV-cache), що залежить від довжини вхідної послідовності.
\end{itemize}

Обчислювальна складність генерації одного токена (в операціях FLOPs) для стандартної архітектури Transformer наближено дорівнює:

\begin{equation}
C \approx 2 \times P \quad [\text{FLOPs/token}]
\end{equation}

Це означає, що для моделі Llama-3-8B генерація одного слова потребує приблизно 16 мільярдів операцій з плаваючою комою. Використання цілочисельного квантування (INT8/INT4) дозволяє замінити операції типу \textit{floating-point} на менш ресурсомісткі \textit{integer} операції, що математично виражається через прискорення обчислень у $N$ разів, де $N$ — коефіцієнт ефективності векторних інструкцій процесора (наприклад, AVX-512 або Apple NEON).

\subsection{Пропускна здатність пам'яті як системне обмеження}

Критичним показником для системного аналізу є не лише об'єм, а й швидкість доступу до даних. Теоретична швидкість генерації ($T_{gen}$) обмежена пропускною здатністю пам'яті (Memory Bandwidth):

\begin{equation}
T_{gen} = \frac{BW}{M_{weights}} \quad [\text{токенів/сек}]
\end{equation}

де $BW$ — пропускна здатність шини пам'яті (ГБ/сек), а $M_{weights}$ — вага моделі в ГБ. Ця формула пояснює, чому квантування в 4 рази (з FP16 до INT4) не лише економить місце, а й потенційно в 4 рази прискорює генерацію тексту на пристроях з повільною RAM.

\subsection{Методологія та інструментарій дослідження}

Експериментальне тестування проводилося на локальній обчислювальній системі з використанням спеціально розробленого програмного інструментарію на мові Python. Скрипт інтегрувався з API сервера Ollama для отримання метаданих моделей та використовував системні бібліотеки \texttt{psutil} та \texttt{GPUtil} для фіксації пікових показників у режимі реального часу.

Об'єктом дослідження виступили моделі різних архітектур (Llama, Qwen, Mistral, Gemma) з об'ємом параметрів від 1B до 8.2B, що використовують 4-бітне квантування (\textit{quantization level} Q4\_K\_M або Q4\_0).

\subsection{Експериментальні дані та аналіз результатів}

Для збору експериментальних даних та верифікації теоретичних розрахунків було розроблено програмний інструментарій на мові Python. Програма здійснює моніторинг апаратних ресурсів (CPU, GPU, RAM) у реальному часі через взаємодію з системними метриками та API сервера Ollama. Повний текст програмного коду бенчмарку наведено у додатку А.

У ході бенчмаркінгу було зафіксовано показники, наведені у таблиці \ref{table:llm_benchmark}.

\begin{table}[h!]
    \small
    \centering
    \caption{Результати аналізу споживання системних ресурсів локальними LLM}
    \label{table:llm_benchmark}
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Модель} & \textbf{Параметри} & \textbf{Квант.} & \textbf{CPU (\%)} & \textbf{GPU (\%)} & \textbf{VRAM (МБ)} \\ \hline
        TinyLlama & 1B & Q4\_0 & 8.0 & 90.0 & 3341 \\ \hline
        Gemma3 & 4.3B & Q4\_K\_M & 36.3 & 40.0 & 3341 \\ \hline
        Qwen3 & 4.0B & Q4\_K\_M & 49.9 & 97.0 & 3444 \\ \hline
        Mistral & 7.2B & Q4\_K\_M & 46.9 & 95.0 & 3389 \\ \hline
        Llama 3.1 & 8.0B & Q4\_K\_M & 49.8 & 100.0 & 3339 \\ \hline
        Qwen3 & 8.2B & Q4\_K\_M & 59.4 & 37.0 & 3494 \\ \hline
        DeepSeek-R1 & 8.2B & Q4\_K\_M & 51.4 & 38.0 & 3318 \\ \hline
    \end{tabular}
\end{table}

\subsection{Інтерпретація результатів аналізу}

На основі отриманих даних (рис. \ref{fig:resource_chart}) було виявлено кілька ключових закономірностей, що мають критичне значення для аналізу:

\begin{enumerate}
    \item \textbf{Стабілізація об'єму VRAM:} Спостерігається відносна сталість зайнятої відеопам'яті (близько 3.3--3.5 ГБ) незалежно від кількості параметрів. Це вказує на агресивне управління пам'яттю в середовищі Ollama та ефективність 4-бітного квантування, яке дозволяє уніфікувати вимоги до сховища для моделей середнього класу.
    
    \item \textbf{Ефект «обчислювального порогу» (Offloading):} Найбільш показовим є результат моделей Qwen3:8b та DeepSeek-R1:8b. Попри те, що вони належать до того ж вагового класу, що й Llama 3.1, навантаження на GPU різко падає до 37--38\%, тоді як навантаження на CPU зростає до максимуму (59.4\%). Це свідчить про автоматичне перенесення обчислювальних шарів у системну RAM через вичерпання ресурсів швидкої пам'яті GPU, що призводить до суттєвого падіння продуктивності.
    
    \item \textbf{Оптимізація архітектур:} Модель Llama 3.1:8b продемонструвала найвищий рівень утилізації GPU (100\%), що робить її еталонною з точки зору балансу між кількістю параметрів та ефективністю використання апаратного прискорення.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar,
            symbolic x coords={TinyLlama, Gemma3:4b, Mistral:7b, Llama3.1:8b, Qwen3:8b},
            xtick=data,
            x tick label style={rotate=45,anchor=east},
            ylabel={Навантаження (\%)},
            legend style={at={(0.5,-0.5)}, anchor=north, legend columns=-1},
            width=12cm, height=7cm,
            ymin=0, ymax=110,
            bar width=15pt
        ]
            \addplot [fill=blue!40] coordinates {(TinyLlama, 8.0) (Gemma3:4b, 36.3) (Mistral:7b, 46.9) (Llama3.1:8b, 49.8) (Qwen3:8b, 59.4)};
            \addplot [fill=red!40] coordinates {(TinyLlama, 90.0) (Gemma3:4b, 40.0) (Mistral:7b, 95.0) (Llama3.1:8b, 100.0) (Qwen3:8b, 37.0)};
            \legend{CPU Load, GPU Load}
        \end{axis}
    \end{tikzpicture}
    \caption{Порівняльний розподіл навантаження на апаратні ресурси системи}
    \label{fig:resource_chart}
\end{figure}

Таким чином, проведений аналіз підтверджує, що для успішного розгортання моделей об'ємом понад 8B на споживчих пристроях стандартного квантування (Q4\_K\_M) недостатньо. Це обґрунтовує необхідність дослідження методів інтенсивнішої оптимізації.

\section{Огляд методів стиснення та оптимізації моделей}
% Квантування, прунінг, дистиляція.

\section{Ефективне донавчання в умовах обмежених ресурсів}
% LoRA, QLoRA.

\section{Огляд програмних рішень для запуску оптимізованих моделей}
% llama.cpp, vLLM тощо.


% --- СПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ ---

\newpage
\chapter*{СПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ}
\addcontentsline{toc}{chapter}{Список використаних джерел}

\begin{enumerate}
    % Приклад оформлення книги з одним автором (згідно з Додатком Ж методички)
    \item Alammar J., Grootendorst M. Hands-On Large Language Models: Language Understanding and Generation. O'Reilly Media, 2024. 412 p.
    \item Vaswani A., Shazeer N., Parmar N. et al. Attention Is All You Need. arXiv:1706.03762 [cs.CL]. 2017. URL: \url{https://arxiv.org/abs/1706.03762} (дата звернення: 17.02.2026).
    \item Transforming agency: On the mode of existence of large language models - Scientific Figure on ResearchGate. URL: \url{https://www.researchgate.net/figure/Schematic-of-the-GPT-3-processing-architecture-as-a-standardized-reference-for-LLM_fig1_394849833} (дата звернення: 17.02.2026)
    \item Jacob B., Kligys S., Chen B. et al. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. arXiv:1712.05877 [cs.LG]. 2017. URL: \url{https://arxiv.org/abs/1712.05877} (дата звернення: 18.02.2026).
    \item Cheng Y., Wang D., Zhou P., Zhang T. A Survey of Model Compression and Acceleration for Deep Neural Networks. arXiv:1710.09282 [cs.LG]. 2020. URL: \url{https://arxiv.org/abs/1710.09282} (дата звернення: 18.02.2026).
    \item Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei. The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. arXiv:2402.17764 [cs.CL]. 2024. URL: \url{https://arxiv.org/abs/2402.17764} (дата звернення: 23.02.2026).
    
    % Приклад оформлення електронного ресурсу (URL)
    % \item Про вищу освіту: Закон України від 01.07.2014 р. № 1556-VII. Дата оновлення: 28.09.2017. URL: http://zakon2.rada.gov.ua/laws/show/1556-18 (дата звернення: 15.11.2017).
\end{enumerate}

\newpage
\appendix
\section*{ДОДАТОК А}
\addcontentsline{toc}{section}{ДОДАТОК А. Програмний код модуля моніторингу системних ресурсів}
\begin{center}
    \textbf{Програмний код модуля моніторингу системних ресурсів}
\end{center}

\begingroup
\enablehyphens
\begin{lstlisting}[language=Python, caption={Скрипт для автоматизованого бенчмаркінгу LLM}]
import psutil
import GPUtil
import time
import requests
import pandas as pd
import threading

OLLAMA_URL = "http://localhost:11434/api"

def get_system_stats():
    cpu = psutil.cpu_percent()
    ram = psutil.virtual_memory().percent
    gpu_load, vram_used = 0, 0
    gpus = GPUtil.getGPUs()
    if gpus:
        gpu_load = gpus[0].load * 100
        vram_used = gpus[0].memoryUsed
    return cpu, ram, gpu_load, vram_used

def get_model_details(model_name):
    try:
        resp = requests.post(f"{OLLAMA_URL}/show", json={"name": model_name})
        data = resp.json()
        details = data.get('details', {})
        return {
            "size": details.get('parameter_size', 'N/A'),
            "quant": details.get('quantization_level', 'N/A'),
            "family": details.get('family', 'N/A')
        }
    except:
        return {"size": "N/A", "quant": "N/A", "family": "N/A"}

def benchmark():
    models_resp = requests.get(f"{OLLAMA_URL}/tags").json()
    model_list = [m['name'] for m in models_resp['models']]
    results = []

    for model_name in model_list:
        print(f"\n>>> Тестування: {model_name}")
        details = get_model_details(model_name)
        
        # Контейнер для пікових значень
        peaks = {"cpu": 0, "ram": 0, "gpu": 0, "vram": 0}
        stop_monitor = False

        def monitor():
            while not stop_monitor:
                c, r, g, v = get_system_stats()
                peaks["cpu"] = max(peaks["cpu"], c)
                peaks["ram"] = max(peaks["ram"], r)
                peaks["gpu"] = max(peaks["gpu"], g)
                peaks["vram"] = max(peaks["vram"], v)
                time.sleep(0.5)

        # Запускаємо моніторинг у фоні
        monitor_thread = threading.Thread(target=monitor)
        monitor_thread.start()

        try:
            # Збільшений таймаут до 300 секунд для важких моделей як DeepSeek
            requests.post(f"{OLLAMA_URL}/generate", 
                          json={"model": model_name, "prompt": "Why is system analysis important?", "stream": False},
                          timeout=300)
        except Exception as e:
            print(f"    Помилка: {e}")
        finally:
            stop_monitor = True
            monitor_thread.join()

        print(f"    [{model_name}] Quant: {details['quant']} | Peak GPU: {peaks['gpu']}% | VRAM: {peaks['vram']}MB")
        
        results.append({
            "Model": model_name,
            "Family": details['family'],
            "Params": details['size'],
            "Quantization": details['quant'],
            "Peak_CPU_%": peaks["cpu"],
            "Peak_RAM_%": peaks["ram"],
            "Peak_GPU_%": peaks["gpu"],
            "Peak_VRAM_MB": peaks["vram"]
        })

    # Збереження результатів
    df = pd.DataFrame(results)
    df.to_csv("llm_system_analysis_results.csv", index=False)
    print("\n[Готово] Дані збережено в llm_system_analysis_results.csv")

if __name__ == "__main__":
    benchmark()
\end{lstlisting}
\endgroup

\end{document}